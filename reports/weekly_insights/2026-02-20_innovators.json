{
  "date": "2026-02-20",
  "segment": "innovators",
  "trends": {
    "categories": {
      "Emerging Tech": 11,
      "Tech Innovation": 2,
      "Management & Leadership": 1
    },
    "sources": {
      "AI News & Artificial Intelligence | TechCrunch": 11,
      "Ars Technica - All content": 2,
      "MIT Sloan Management Review": 1
    },
    "topics": [
      "OpenAI reportedly finalizing $100B deal at more than $850B valuation",
      "Reliance unveils $110B AI investment plan as India ramps up tech ambitions",
      "Freeform raises $67M Series B to scale up laser AI manufacturing",
      "Co-founders behind Reface and Prisma join hands to improve on-device model inference with Mirai",
      "Microsoft's new 10,000-year data storage medium: glass",
      "Record scratch\u2014Google's Lyria 3 AI music model is coming to Gemini today",
      "Reload wants to give your AI agents a shared memory",
      "For open source programs, AI coding tools are a mixed blessing",
      "OpenAI deepens India push with Pine Labs fintech partnership",
      "OpenAI, Reliance partner to add AI search to JioHotstar",
      "OpenAI taps Tata for 100MW AI data center capacity in India, eyes 1GW",
      "Google Cloud\u2019s VP for startups on reading your \u2018check engine light\u2019 before\u00a0it\u2019s\u00a0too late",
      "Altman and Amodei share a moment of awkwardness at India\u2019s big AI summit",
      "Connecting Language and (Artificial) Intelligence: Princeton\u2019s Tom Griffiths"
    ],
    "total_articles": 14
  },
  "article_count": 14,
  "articles": [
    {
      "id": "c6f24519cc1f7de28eb53ef9b63fd88e",
      "title": "OpenAI reportedly finalizing $100B deal at more than $850B valuation",
      "url": "https://techcrunch.com/2026/02/19/openai-reportedly-finalizing-100b-deal-at-more-than-850b-valuation/",
      "published_date": "2026-02-20T06:56:12.142737",
      "description": "OpenAI is reportedly getting close to closing a $100 billion deal, with backers including Amazon, Nvidia, SoftBank, and Microsoft. The deal would value the ChatGPT-maker at $850 billion.",
      "source": "AI News & Artificial Intelligence | TechCrunch",
      "category": "Emerging Tech",
      "segment": "innovators",
      "source_type": "secondary",
      "raw_content": "OpenAI is reportedly getting close to closing a $100 billion deal, with backers including Amazon, Nvidia, SoftBank, and Microsoft. The deal would value the ChatGPT-maker at $850 billion.",
      "tier": "full",
      "selection_reason": "Major AI industry shift with unprecedented valuation and strategic partnerships",
      "audience_value": "Critical insights into AI industry leadership and investment trends",
      "urgency_score": 10,
      "category_tag": "\ud83d\ude80 AI & Innovation"
    },
    {
      "id": "ab54e45f9f7c71d2dccfee8672ad2813",
      "title": "Reliance unveils $110B AI investment plan as India ramps up tech ambitions",
      "url": "https://techcrunch.com/2026/02/19/reliance-unveils-110b-ai-investment-plan-as-india-ramps-up-tech-ambitions/",
      "published_date": "2026-02-20T06:56:12.142915",
      "description": "Reliance has begun building multi-gigawatt AI data centers in Jamnagar, with more than 120 MW of capacity expected to come online in 2026.",
      "source": "AI News & Artificial Intelligence | TechCrunch",
      "category": "Emerging Tech",
      "segment": "innovators",
      "source_type": "secondary",
      "raw_content": "Reliance has begun building multi-gigawatt AI data centers in Jamnagar, with more than 120 MW of capacity expected to come online in 2026.",
      "tier": "full",
      "selection_reason": "Massive AI infrastructure investment reshaping global tech landscape",
      "audience_value": "Strategic insights into emerging AI compute markets and infrastructure scaling",
      "urgency_score": 9,
      "category_tag": "\ud83d\udcbc Tech Business"
    },
    {
      "id": "ff91d91cd035923191c9c3c73befa2fb",
      "title": "Freeform raises $67M Series B to scale up laser AI manufacturing",
      "url": "https://techcrunch.com/2026/02/19/freeform-raises-67m-series-b-to-scale-up-laser-ai-manufacturing/",
      "published_date": "2026-02-20T06:56:12.142893",
      "description": "\u201cI think we're the only quote-unquote manufacturing company out there that has H200 clusters in a data center on site.\"",
      "source": "AI News & Artificial Intelligence | TechCrunch",
      "category": "Emerging Tech",
      "segment": "innovators",
      "source_type": "secondary",
      "raw_content": "\u201cI think we're the only quote-unquote manufacturing company out there that has H200 clusters in a data center on site.\"",
      "tier": "full",
      "selection_reason": "Novel intersection of AI and manufacturing technology",
      "audience_value": "Insights into practical AI applications in advanced manufacturing",
      "urgency_score": 8,
      "category_tag": "\ud83d\ude80 AI & Innovation"
    },
    {
      "id": "824b0834f6f43de4b6bdc9a1ba6a1d28",
      "title": "Co-founders behind Reface and Prisma join hands to improve on-device model inference with Mirai",
      "url": "https://techcrunch.com/2026/02/19/co-founders-behind-reface-and-prisma-join-hands-to-improve-on-device-model-inference-with-mirai/",
      "published_date": "2026-02-20T06:56:12.142805",
      "description": "Mirai raised a $10 million seed round to improve how AI models run on devices like smartphones and laptops.",
      "source": "AI News & Artificial Intelligence | TechCrunch",
      "category": "Emerging Tech",
      "segment": "innovators",
      "source_type": "secondary",
      "raw_content": "Mirai raised a $10 million seed round to improve how AI models run on devices like smartphones and laptops.",
      "tier": "full",
      "selection_reason": "Breakthrough in on-device AI model optimization",
      "audience_value": "Technical insights into edge AI deployment strategies",
      "urgency_score": 8,
      "category_tag": "\ud83d\ude80 AI & Innovation"
    },
    {
      "id": "5c1a3aec0170f34cda0e124d42faba4e",
      "title": "Microsoft's new 10,000-year data storage medium: glass",
      "url": "https://arstechnica.com/science/2026/02/microsofts-new-10000-year-data-storage-medium-glass/",
      "published_date": "2026-02-20T06:56:11.271515",
      "description": "Femtosecond lasers etch data into a very stable medium.",
      "source": "Ars Technica - All content",
      "category": "Tech Innovation",
      "segment": "innovators",
      "source_type": "secondary",
      "raw_content": "<p>Archival storage poses lots of challenges. We want media that is extremely dense and stable for centuries or more, and, ideally, doesn't consume any energy when not being accessed. Lots of ideas have floated around\u2014<a href=\"https://arstechnica.com/science/2021/06/researchers-build-a-metadata-based-image-database-using-dna-storage/\">even DNA has been considered</a>\u2014but one of the simplest is to cut the data into glass. Many forms of glass are very physically and chemically stable, and it's relatively easy to create features in it.</p>\n<p>There's been a lot of preliminary work demonstrating different aspects of a glass-based storage system. But in Wednesday's issue of Nature, Microsoft Research announced Project Silica, a working demonstration of a system that can read and write data into small slabs of glass with a density of over a Gigabit per cubic millimeter.</p>\n<h2>Writing on glass</h2>\n<p>We tend to think of glass as fragile, prone to shattering, and capable of flowing downward over centuries, although the last claim is a myth. Glass is a category of material, and a variety of chemicals can form glasses. With the right starting chemical, it's possible to make a glass that is, as the researchers put it, \"thermally and chemically stable and is resistant to moisture ingress, temperature fluctuations and electromagnetic interference.\" While it would still need to be handled in a way to minimize damage, glass provides the sort of stability we'd want for long-term\u00a0storage.</p><p><a href=\"https://arstechnica.com/science/2026/02/microsofts-new-10000-year-data-storage-medium-glass/\">Read full article</a></p>\n<p><a href=\"https://arstechnica.com/science/2026/02/microsofts-new-10000-year-data-storage-medium-glass/#comments\">Comments</a></p>",
      "tier": "full",
      "selection_reason": "Revolutionary data storage technology for long-term preservation",
      "audience_value": "Insights into next-generation data storage solutions",
      "urgency_score": 8,
      "category_tag": "\ud83d\ude80 AI & Innovation"
    },
    {
      "id": "772f766b7dd6c7565121d4b3eb84a86f",
      "title": "Record scratch\u2014Google's Lyria 3 AI music model is coming to Gemini today",
      "url": "https://arstechnica.com/google/2026/02/gemini-can-now-generate-ai-music-for-you-no-lyrics-required/",
      "published_date": "2026-02-20T06:56:11.271563",
      "description": "With a simple prompt, you can generate 30 seconds of something like music.",
      "source": "Ars Technica - All content",
      "category": "Tech Innovation",
      "segment": "innovators",
      "source_type": "secondary",
      "raw_content": "<p>The American poet Henry Wadsworth Longfellow called music \"the universal language of mankind.\" Is that still true when the so-called music is being generated by a probabilistic robot instead of a human? We're about to find out. Google has announced its latest Lyria 3 AI model is being <a href=\"https://blog.google/innovation-and-ai/products/gemini-app/lyria-3/\">deployed in the Gemini app</a>, vastly expanding access to AI music generation.</p>\n<p>Google DeepMind has been tinkering with Lyria for a while now, offering limited access in developer-oriented products like Vertex AI. Lyria 3 is more capable than previous versions, and it's also quicker to use. Just select the new \"Create music\" option in the Gemini app or web UI to get started. You can describe what you want and even upload an image to help the robot get the right vibe. And in a few seconds, you get music (or something like it).</p>\n<p>In case there was any uncertainty about whether Lyria tracks still counted as a human artistic endeavor, worry not! Unlike past versions of the model, you don't even have to provide lyrics in your prompt. You can be vague with your request, and the model will create suitable lyrics for the 30-second song. Although with that limit, \"jingle\" might be more accurate.</p><p><a href=\"https://arstechnica.com/google/2026/02/gemini-can-now-generate-ai-music-for-you-no-lyrics-required/\">Read full article</a></p>\n<p><a href=\"https://arstechnica.com/google/2026/02/gemini-can-now-generate-ai-music-for-you-no-lyrics-required/#comments\">Comments</a></p>",
      "tier": "full",
      "selection_reason": "Major advancement in AI-generated music capabilities",
      "audience_value": "Understanding new frontiers in creative AI applications",
      "urgency_score": 9,
      "category_tag": "\ud83d\ude80 AI & Innovation"
    },
    {
      "id": "11b7b9ecdbbf6d639ed49b841b581ab7",
      "title": "Reload wants to give your AI agents a shared memory",
      "url": "https://techcrunch.com/2026/02/19/reload-an-ai-employee-agent-management-platform-raises-2-275m-and-launches-an-ai-employee/",
      "published_date": "2026-02-20T06:56:12.142761",
      "description": "Reload announces a $2.275 million raise in a round led by Anthemis and the launch of its first AI employee, Epic.",
      "source": "AI News & Artificial Intelligence | TechCrunch",
      "category": "Emerging Tech",
      "segment": "innovators",
      "source_type": "primary",
      "raw_content": "Reload announces a $2.275 million raise in a round led by Anthemis and the launch of its first AI employee, Epic.",
      "tier": "full",
      "selection_reason": "Novel approach to AI agent memory and coordination",
      "audience_value": "Technical insights into advanced AI agent architectures",
      "urgency_score": 8,
      "category_tag": "\ud83d\ude80 AI & Innovation"
    },
    {
      "id": "48d0bb685c55d451f048cdfef06dc3fa",
      "title": "For open source programs, AI coding tools are a mixed blessing",
      "url": "https://techcrunch.com/2026/02/19/for-open-source-programs-ai-coding-tools-are-a-mixed-blessing/",
      "published_date": "2026-02-20T06:56:12.142845",
      "description": "AI coding tools have enabled a flood of bad code that threatens to overwhelm many projects. Building new features is easier, but maintaining them is just as hard.",
      "source": "AI News & Artificial Intelligence | TechCrunch",
      "category": "Emerging Tech",
      "segment": "innovators",
      "source_type": "secondary",
      "raw_content": "AI coding tools have enabled a flood of bad code that threatens to overwhelm many projects. Building new features is easier, but maintaining them is just as hard.",
      "tier": "full",
      "selection_reason": "Critical analysis of AI's impact on software development",
      "audience_value": "Practical insights into AI coding tool integration challenges",
      "urgency_score": 8,
      "category_tag": "\ud83d\ude80 AI & Innovation"
    },
    {
      "id": "bf058bbbcc75fac70c3a1f585c708e49",
      "title": "OpenAI deepens India push with Pine Labs fintech partnership",
      "url": "https://techcrunch.com/2026/02/18/openai-deepens-india-push-with-pine-labs-fintech-partnership/",
      "published_date": "2026-02-20T06:56:12.142960",
      "description": "OpenAI moves beyond ChatGPT in India with a Pine Labs deal targeting enterprise payments and AI-driven commerce.",
      "source": "AI News & Artificial Intelligence | TechCrunch",
      "category": "Emerging Tech",
      "segment": "innovators",
      "source_type": "secondary",
      "raw_content": "OpenAI moves beyond ChatGPT in India with a Pine Labs deal targeting enterprise payments and AI-driven commerce.",
      "tier": "quick",
      "selection_reason": "Strategic expansion of AI capabilities in emerging markets",
      "audience_value": "Market expansion insights for AI technologies",
      "urgency_score": 7,
      "category_tag": "\ud83d\udcbc Tech Business"
    },
    {
      "id": "b93fa417b80b0180c7a6d57751014013",
      "title": "OpenAI, Reliance partner to add AI search to JioHotstar",
      "url": "https://techcrunch.com/2026/02/19/openai-reliance-partner-to-add-ai-search-to-jiohotstar/",
      "published_date": "2026-02-20T06:56:12.142783",
      "description": "The rollout includes two-way integration that surfaces streaming links directly inside ChatGPT.",
      "source": "AI News & Artificial Intelligence | TechCrunch",
      "category": "Emerging Tech",
      "segment": "innovators",
      "source_type": "secondary",
      "raw_content": "The rollout includes two-way integration that surfaces streaming links directly inside ChatGPT.",
      "tier": "quick",
      "selection_reason": "Novel AI integration in streaming media",
      "audience_value": "Practical application of AI in consumer services",
      "urgency_score": 7,
      "category_tag": "\ud83d\ude80 AI & Innovation"
    },
    {
      "id": "f440fde9418f8f5db816f345c01a0649",
      "title": "OpenAI taps Tata for 100MW AI data center capacity in India, eyes 1GW",
      "url": "https://techcrunch.com/2026/02/18/openai-taps-tata-for-100mw-ai-data-center-capacity-in-india-eyes-1gw/",
      "published_date": "2026-02-20T06:56:12.142938",
      "description": "OpenAI also plans to expand its presence in India with new offices in Mumbai and Bengaluru later this year.",
      "source": "AI News & Artificial Intelligence | TechCrunch",
      "category": "Emerging Tech",
      "segment": "innovators",
      "source_type": "secondary",
      "raw_content": "OpenAI also plans to expand its presence in India with new offices in Mumbai and Bengaluru later this year.",
      "tier": "quick",
      "selection_reason": "Major AI infrastructure development in emerging markets",
      "audience_value": "Strategic insights into global AI compute expansion",
      "urgency_score": 7,
      "category_tag": "\ud83d\udcbc Tech Business"
    },
    {
      "id": "25fa047a53112e3719d2190fffa44b82",
      "title": "Google Cloud\u2019s VP for startups on reading your \u2018check engine light\u2019 before\u00a0it\u2019s\u00a0too late",
      "url": "https://techcrunch.com/podcast/google-clouds-vp-for-startups-on-reading-your-check-engine-light-before-its-too-late/",
      "published_date": "2026-02-20T06:56:12.143005",
      "description": "Startup founders are being pushed to move faster than ever, using AI while facing tighter funding, rising infrastructure costs, and more pressure to show real traction early. Cloud credits, access to GPUs, and foundation models have made it easier to get started, but those early infrastructure choices can have unforeseen consequences once startups move beyond [&#8230;]",
      "source": "AI News & Artificial Intelligence | TechCrunch",
      "category": "Emerging Tech",
      "segment": "innovators",
      "source_type": "secondary",
      "raw_content": "Startup founders are being pushed to move faster than ever, using AI while facing tighter funding, rising infrastructure costs, and more pressure to show real traction early. Cloud credits, access to GPUs, and foundation models have made it easier to get started, but those early infrastructure choices can have unforeseen consequences once startups move beyond [&#8230;]",
      "tier": "quick",
      "selection_reason": "Strategic insights into AI infrastructure planning",
      "audience_value": "Early warning signs for AI infrastructure scaling",
      "urgency_score": 6,
      "category_tag": "\u2601\ufe0f Enterprise Tech"
    },
    {
      "id": "d0776eaa63f2e635e0f3a44d6a5962f6",
      "title": "Altman and Amodei share a moment of awkwardness at India\u2019s big AI summit",
      "url": "https://techcrunch.com/2026/02/19/altman-and-amodei-share-a-moment-of-awkwardness-at-indias-big-ai-summit/",
      "published_date": "2026-02-20T06:56:12.142870",
      "description": "When Prime Minister Narendra Modi prompted speakers at the event to join hands and raise them in a show of unity, all executives onstage obliged, except OpenAI's Sam Altman and Anthropic's Dario Amodei, who held their hands conspicuously apart.",
      "source": "AI News & Artificial Intelligence | TechCrunch",
      "category": "Emerging Tech",
      "segment": "innovators",
      "source_type": "secondary",
      "raw_content": "When Prime Minister Narendra Modi prompted speakers at the event to join hands and raise them in a show of unity, all executives onstage obliged, except OpenAI's Sam Altman and Anthropic's Dario Amodei, who held their hands conspicuously apart.",
      "tier": "trending",
      "selection_reason": "Industry dynamics between major AI players",
      "audience_value": "Insights into AI industry relationships and tensions",
      "urgency_score": 7,
      "category_tag": "\ud83d\udcca Market Trends"
    },
    {
      "id": "eecbbbfa60a2d5f005f763fb85561f57",
      "title": "Connecting Language and (Artificial) Intelligence: Princeton\u2019s Tom Griffiths",
      "url": "https://sloanreview.mit.edu/audio/connecting-language-and-artificial-intelligence-princetons-tom-griffiths/",
      "published_date": "2026-02-20T06:55:49.111428",
      "description": "In this bonus episode of the Me, Myself, and AI podcast, Princeton University professor and artificial intelligence researcher Tom Griffiths joins host Sam Ransbotham to unpack The Laws of Thought, his new book exploring how math has been used for centuries to understand how minds \u2014 human and machine \u2014 actually work. Tom walks through [&#8230;]",
      "source": "MIT Sloan Management Review",
      "category": "Management & Leadership",
      "segment": "leaders",
      "source_type": "secondary",
      "raw_content": "<p></p>\n<p>In this bonus episode of the <cite>Me, Myself, and AI</cite> podcast, Princeton University professor and artificial intelligence researcher Tom Griffiths joins host Sam Ransbotham to unpack <cite>The Laws of Thought</cite>, his new book exploring how math has been used for centuries to understand how minds \u2014 human and machine \u2014 actually work. Tom walks through three main frameworks shaping intelligence today \u2014 rules and symbols, neural networks, and probability \u2014 and he explains why modern AI only makes sense when you see how those pieces fit together. The conversation connects cognitive science, large language models, and the limits of human versus machine intelligence. Along the way, Tom and Sam dig into language, learning, and what humans still do better \u2014 like judgment, curation, and metacognition.</p>\n<aside class=\"callout-info\">\n<img alt=\"Tom Griffiths\" src=\"https://sloanreview.mit.edu/wp-content/uploads/2025/12/MMAI-S12-BONUS1-Griffiths-Princeton-headshot-600.jpg\" /></p>\n<h4>Tom Griffiths, Princeton University</h4>\n<p>Tom Griffiths is the author of the new book <cite>The Laws of Thought: The Quest for Mathematical Theory of the Mind</cite> and the Henry R. Luce Professor of Information Technology, Consciousness, and Culture at Princeton University. He also directs Princeton\u2019s Computational Cognitive Science Lab, a research group focused on understanding the mathematical foundations of human cognition, and the Princeton Laboratory for Artificial Intelligence, which supports innovative research efforts in AI and related fields. Griffiths coauthored the book <cite>Algorithms to Live By</cite>, and his award-winning research has been published in <cite>Science</cite>, <cite>Nature</cite>, and the <cite>Proceedings of the National Academy of Sciences</cite>.</p>\n</aside>\n<p>Subscribe to <cite>Me, Myself, and AI</cite> on <a href=\"https://podcasts.apple.com/us/podcast/me-myself-and-ai/id1533115958\" rel=\"noopener\" target=\"_blank\">Apple Podcasts</a> or <a href=\"https://open.spotify.com/show/7ysPBcYtOPVgI6W5an6lup\" rel=\"noopener\" target=\"_blank\">Spotify</a>.</p>\n<h4>Transcript</h4>\n<p><strong>Allison Ryder:</strong> Hi, everyone. While we\u2019re on winter break, we\u2019re dropping a couple of bonus episodes featuring cutting-edge academic researchers. On today\u2019s episode, Sam is joined by professor and director of Princeton\u2019s Computational Cognitive Science Lab, Tom Griffiths. Tom is the author of the forthcoming book <cite>The Laws of Thought</cite> and joins Sam today to speak about AI\u2019s mathematical and linguistic backgrounds. It was a fascinating conversation, and I hope you enjoy it. </p>\n<p><strong>Tom Griffiths:</strong> I\u2019m Tom Griffiths from Princeton University, and you\u2019re listening to <cite>Me, Myself, and AI</cite>. </p>\n<p><strong>Sam Ransbotham:</strong> Welcome to <cite>Me, Myself, and AI</cite>, a podcast from <cite>MIT Sloan Management Review</cite> exploring the future of artificial intelligence. I\u2019m Sam Ransbotham, professor of analytics at Boston College. I\u2019ve been researching data, analytics, and AI at <cite>MIT SMR</cite> since 2014, with research articles, annual industry reports, case studies, and now 12 seasons of podcast episodes. In each episode, corporate leaders, cutting-edge researchers, and AI policy makers join us to break down what separates AI hype from AI success.</p>\n<p>Hi, listeners. Thanks, everyone, for joining us again. Our guest today is Tom Griffiths, professor of psychology and computer science, and the director of the Princeton Laboratory for Artificial Intelligence. Tom has a new book, <cite>The Laws of Thought</cite>, which I suspect our listeners will enjoy learning about. Tom, [it\u2019s] great to have you on the podcast.</p>\n<p><strong>Tom Griffiths:</strong> Thanks, Sam. [It\u2019s] great to be here. </p>\n<p><strong>Sam Ransbotham:</strong> Why don\u2019t we start with \u2026 I think people know it\u2019s kind of fun to be your professor because people know what professors are, but maybe let\u2019s start with a little bit of a bio. Can you give us some background on what your roles are with the lab at Princeton? </p>\n<p><strong>Tom Griffiths:</strong> Princeton, like a lot of other educational institutions, has been trying to figure out how to respond to all of the things that are happening with AI in the world at the moment. The AI lab is the starting point for doing that in terms of thinking about being able to make some targeted investments in research areas where we see potential for [a] transformative impact for AI in a way that\u2019s maybe more nimble than a traditional academic institution might. </p>\n<p><strong>Sam Ransbotham:</strong> There\u2019s a lot going on within universities trying to figure out what exactly all this means, and I guess all of society. But let\u2019s start with <cite>The Laws of Thought</cite>. Can you explain in some simple terms what these laws are and how they relate to human cognition and artificial intelligence? </p>\n<p><strong>Tom Griffiths:</strong> The idea behind the book is \u2026 I think all of us in school learn about the laws of nature, right? These [are] sort of principles of physics or something like that, that tell us about how \u2026 the world around us works. One interesting thing is that the same scientists who hundreds of years ago were trying to figure out what those laws of nature were, using math to describe the physical world, were just as interested in using math to try and understand the mental world, the world inside us. The book is really the story of that effort. </p>\n<p>It turns out understanding our inside world is a bit harder than understanding our outside world. It took us a little bit longer to figure out what the fundamental principles are. It charts the story, from people first introducing this idea of using mathematics to understand the mind, through some of the first discoveries about what kinds of mathematical principles could be used for explaining how minds work \u2014 things like mathematical logic \u2014 to the discovery that that was not going to get us all the way to understanding things like how people learn complex concepts that have fuzzy boundaries, things like languages, and then ideas like artificial neural networks, which are very popular at the moment in artificial intelligence, and then probability and statistics as another approach that really helps us understand why it is that some of those AI methods actually work. </p>\n<p><strong>Sam Ransbotham:</strong> I think you approach this from three different frameworks: rules and symbols is one framework, neural networks is another, and then Bayesian probability is a third. Maybe I\u2019m grossly oversimplifying these three big prongs in the book. Maybe take a minute and explain what each of those pieces are and then, more importantly, how they all weave together. </p>\n<p><strong>Tom Griffiths:</strong> You got it. Those are the three big pieces. Basically the story is that I think the origins of people trying to think about mathematical principles for understanding the mind are really tied up in that rules and symbols approach. That was because that seemed like the first tool that we had that really described something like how thought worked. So if you go back to the origins of logic, the title of the book, <cite>The Laws of Thought</cite>, is a phrase that\u2019s used by George Boole, who was working in the 19th century and sort of figured out some of the first principles of mathematical logic. </p>\n<p>Those principles turned into the principles that underlie our computers today, through the work of Alan Turing and John von Neumann and others. When psychologists were trying to work out how to rigorously study something that you can\u2019t see or touch \u2014 something inside our heads, our minds \u2014 they discovered that those mathematical principles of logic were actually really useful for expressing rigorous, precise hypotheses about how minds work. So that was the starting point for what we now call cognitive science, which is trying to use these mathematical principles to figure out how minds work. </p>\n<p>For a while it seemed like that was going pretty well. It turned out that those systems of rules and symbols worked well for describing things like deductive reasoning, things like problem-solving or planning, things like even the structure of languages through the work of people like Noam Chomsky. But after a while they started to realize that maybe that wasn\u2019t going to be all that we\u2019d need in order to understand how minds work. </p>\n<p>One of the big problems for that rules and symbols approach was explaining learning. It helps us to explain how we reason, helps us to explain what the structures or languages are like, but it doesn\u2019t help us to explain how those things get into our heads in the first place. How do we learn these kinds of strategies for thinking? How do we learn what the structure of those languages are? And it also didn\u2019t work for capturing some of the \u2026 fuzzy boundaries that we see in real concepts. \u2026 If you ask people, \u201cIs an olive a fruit?\u201d people are quite uncertain about [the answer] in a way that\u2019s maybe hard to capture if you\u2019re really just thinking in terms of something like logic. </p>\n<p>In the 1960s-1970s, people started to explore different ways of thinking about the mind in terms of different kinds of mathematics, and thinking about things like maybe our concepts are related to \u2026 we can think about something in the world as being a point in space, where it\u2019s an abstract space that picks out the features that thing has. Maybe a concept is a region in that space. Now a new kind of mathematics is needed for describing these kinds of continuous representations. </p>\n<p>It turns out that when you start thinking in those terms, you end up getting to new ways of thinking about how to solve that learning problem. And that\u2019s where artificial neural networks come in. They\u2019re essentially a way of thinking about how to represent things as points in space and then learn the relationships between those points in space so you can map from one space to another. So that solved a bunch of problems that we had for logic. </p>\n<p>But then we have a bunch of other questions. For example, if we look at things like our large language models [LLMs] today that are very successful in doing all sorts of things like learning language. Really understanding why it is that they\u2019re able to do that requires us to take one more step and think about a different kind of mathematical idea. [Those] ideas come from probability and statistics. Statistics is really the science of inductive inference. It tells us what we can infer from data, and probability theory gives us a tool for understanding how we can work with uncertainty and how we can make inferences from the data that we see. </p>\n<p><strong>Sam Ransbotham:</strong> Is there a fourth one? We\u2019ve got three nice things, and each time you pointed out some aspect of them that [was] strong, and some aspect that led to a limitation. Is there [a] number four out there that we need that we haven\u2019t figured out yet? Or is it just a matter of getting these three mixed in the right proportions and emphasized in the right ways? </p>\n<p><strong>Tom Griffiths:</strong> I think these three are actually pretty good. \u2026 A funny thing that happened to me when I was writing this book is that I\u2019ve been teaching these kinds of ideas to undergraduates for 20 years. When I give my class on computational approaches to understanding cognition, I would normally start that class by saying, \u201cUnlike taking a class in physics or something like that, where you can expect to hear the answers, we\u2019re still figuring these things out. We have good ways of asking the questions [even though] we haven\u2019t quite \u2026 got to those answers.\u201d But I actually think in the last 10 years, in the period that I was working on the book, I think there\u2019s been a change in how much we understand about these things and how well they fit together. We can kind of start to see some glimmers of really figuring out what those laws of thought might look like. </p>\n<p><strong>Sam Ransbotham:</strong> [Are these] laws of thought going to be understanding the borders between these better, or is it going to be some sort of complementarity between them, or some sort of combination in a unique way? </p>\n<p><strong>Tom Griffiths:</strong> Complementarity and combination are, I think, the two ways to think about this. One thing that we\u2019ve started to realize is that there [are] different ways that you can provide an explanation for something like the human mind. So, again, if you\u2019re a physical scientist and you want to explain a phenomenon, say, the behavior of an animal, you could think about explaining that phenomenon at lots of different levels. You could explain it in terms of the environment that the animal is in. You could explain it in terms of the muscles and bones of the animal that are doing certain kinds of things and the nerves and so on. You could explain it in terms of the chemical reactions that are happening to produce those things, or you could explain it in terms of the atoms and molecules that are interacting. There are all of these different levels of analysis that we\u2019re used to thinking about when we think about physical systems. </p>\n<p>One of the insights of cognitive science, something that goes back to a theoretical neuroscientist called David Marr, is the idea that there are similar kinds of levels of analysis that we can think about when we\u2019re trying to understand something like human behavior or the behavior of an intelligent system more generally. </p>\n<p>Marr suggested you could think about this in terms of three levels. The most abstract is what he called the computational level, which is: What\u2019s the abstract problem the system is solving, and what does the solution to that problem look like? And then, more concrete than that, there\u2019s what\u2019s called the algorithmic level, which is: What are the actual processes that are going on inside that system? What [are] the algorithms that are being executed to produce that solution? Then the third is what\u2019s called the implementation level, which is: How is that algorithm implemented inside the brain? </p>\n<p>One important insight here is that these three different systems of mathematics that we\u2019ve been talking about don\u2019t need to be fighting with one another. They can be cooperating by giving us explanations that operate at different levels of analysis. In particular, logic and probability theory are at that most abstract level. They kind of describe how an ideal agent should solve problems that [it] faces, problems like, how do I figure out what\u2019s true based on the things that I already know? Or what inferences can I draw from the things that I\u2019ve seen already? </p>\n<p>The neural networks give us a story about how you can actually create systems that implement different kinds of algorithms that are strategies for approximating solutions to those more abstract, more idealized kinds of mathematical systems. </p>\n<p>So part of the reason why I think maybe three is enough is that logic tells us how to solve what we call deductive problems, problems that require figuring out what\u2019s true when we have all the information. Probability theory tells us how to solve what are called inductive problems, problems where we don\u2019t have all the information and we have to kind of do the best we can to figure out what to do based on what we know. And then our brains somehow solve both of those kinds of problems using things that look very much like the kinds of structures of artificial neural networks. So that combination of three things actually gives us ways of describing the abstract problems we solve as well as the kinds of physical systems that might actually implement solutions to those problems. </p>\n<p><strong>Sam Ransbotham:</strong> Language is a pervasive idea through the book. We can give a nod to my frequent coauthor David Kiron, who\u2019s super into [Ludwig] Wittgenstein and \u201cIf you can\u2019t express it in language, it doesn\u2019t exist\u201d type of ideas. It\u2019s a major function of the book. </p>\n<p>What\u2019s the relationship between maybe the fact that we\u2019re coming up with all this in a moment of English language dominance and would all these things be the same if we\u2019d had them 200 years ago with a French language dominance? We, at the same time, have another layer of mathematical language, which is pervasive. Is coding language another one of these languages that is going to be pervasive? How do all these connections between language and intelligence link up in your mind?</p>\n<p><strong>Tom Griffiths:</strong> Language is a recurring example because it has characteristics that line up with all of these three kinds of mathematical ways of thinking about minds. So the rules and symbols part is what you can think about in terms of a traditional \u2026 way of thinking about grammar in language: A sentence has a noun and a verb, and you know they\u2019re combined in particular ways, and you can move them around in certain kinds of ways. That was this sort of important insight that Noam Chomsky had that really organized most of 20th century linguistics and continues to influence the way people think about these things. </p>\n<p>But that is not enough to explain everything that happens involving language. One of the challenges that Chomsky had was explaining how it is that human children come to speak language, because there wasn\u2019t really a good way to formalize learning in that rules and symbols approach. So he ended up concluding you just had to build it all in. And then, based on the limited information you get, you\u2019ve got enough constraints in the system that the right thing comes out. </p>\n<p>Neural networks offered a way to think about how you could learn things like language from data by showing that even if you had something that was described by a system of rules and symbols, it didn\u2019t need to be implemented as a system of rules and symbols. Some of the key insights that came from [the] early work [of] using neural networks were that you could take grammar and you could have a neural network learn that grammar. It could do so without ever having explicitly represented a noun or a verb or any of those kinds of things. So that gives us a different way of thinking about what language is and a way of understanding how it is that languages might be learned. </p>\n<p>And then, probability theory helps us to understand how, in general, we could imagine those processes of learning working, and sort of understand what it is that they do. So if we think about what language is, it also has this inductive component, where when I say something, you\u2019re trying to figure out what I\u2019m trying to communicate to you, and you\u2019re making an inference from the things that are coming out of my mouth in order to know what\u2019s going on. And your brain is also trying to solve a prediction problem, where it\u2019s trying to cue up, \u201cWhat are the concepts that are likely to come up in this conversation next?\u201d in a way that makes it easy for you to understand the things that we\u2019re saying. </p>\n<p>So if you think about language as a probabilistic object, then we can understand some of the things about how it works and how it\u2019s learned in a way that goes beyond just how we might actually be able to create something like a neural network that instantiates that language. </p>\n<p>When we look at things like large language models, I think they\u2019re actually a great illustration of how these three things come together. First of all, in order to create these systems [that] are able to demonstrate a remarkable amount of intelligence, we need to train them on something [that] has this kind of rules and symbols structure. Large language models aren\u2019t just trained on natural language, and they\u2019re not just trained on English. So you get trained on English. You get trained on French. They also get trained on a large amount of computer code. They\u2019re getting a lot of symbolic structure built into them through that training process. And that\u2019s part of what underlies the intelligence that they manifest. They\u2019re based on these big artificial neural networks. So that\u2019s that second piece. That\u2019s the reason why it\u2019s possible to learn those things. </p>\n<p>And then the third piece is the way that they\u2019re trained is by predicting the next token [that] is going to appear in a sequence, the next word or part of a word that they\u2019re going to see based on all of the words, the parts of words that they\u2019ve seen before. So that training is explicitly setting this up as a probabilistic model. But what it\u2019s trying to do is to learn a probability distribution over sequences of tokens. And what it\u2019s trying to do when you\u2019re interacting with it is make inferences about what sequences of tokens you might want it to generate based on the sequences of tokens that you\u2019ve typed into it. </p>\n<p><strong>Sam Ransbotham:</strong> Let me push back though a little bit. \u2026 You mentioned the rules and symbols. It feels like that\u2019s a place where these current implementations of large language models might be a bit weak. I think one of the examples you gave in your book is that we can pick up a word from the first use, or we can even make up words that should be real words, even if they aren\u2019t really words. We do that based off of an understanding of how those symbols work and how those tokens work. \u2026 People can learn a word on their first hearing. They don\u2019t require the 14 million images of ImageNet to learn. You kind of implied that given this large corpus of knowledge, it would extract those rules and symbols. </p>\n<p>Why not push them a little bit more and give them some rules and symbols to work with? Rather than learn about gravity, you say, \u201cHey, here\u2019s gravity. This is how it works \u2014 always, not just in the four examples that you\u2019ve seen. It works that way all the time.\u201d Where\u2019s that? </p>\n<p>Maybe I\u2019ll push back a little bit to say [that] when I\u2019ve seen some examples of some of the large language models struggling with math, they\u2019ve struggled not with \u201cWhat is 2 plus 2?\u201d Because they\u2019ve seen millions of examples of that. But if you take a superlong number and add it to another superlong number, you tend to get a random, superlong, other number rather than the symbolic representation. You mentioned, I think, in one of the chapters, [that] we understand numbers without ever having seen that specific number before. Is there room for more symbolic processing here, [a] more rules-based approach? </p>\n<p></p>\n<p><strong>Tom Griffiths:</strong> Yeah. I think you highlighted two of the important ways in which current AI systems differ from human cognition, and two of the kinds of things where we can imagine learning things from how human minds work that might make those AI systems better. So those two things [involve] generalization \u2014 being able to generalize in a systematic way beyond the data that they\u2019re provided and learning from small amounts of data. </p>\n<p>I gave the example of kids learning language. A kid learns language on the order of 10 years, whereas the kinds of large language models that are deployed today require more on the order of 10,000 years of continuous speech or something like that in order to reach the level of competence that they reach. </p>\n<p>So those are places where we have opportunities to learn from people. The first of those, this point about generalization, is really about have you formed the right kinds of representations of a domain such that when you start to see things that go beyond the training data you\u2019ve seen, you\u2019re able to then respond to those in ways that are consistent with what you should have learned, in order to represent the domain that you\u2019re operating in? That remains an outstanding problem for language models. </p>\n<p>Part of the way that they\u2019re able to do this is they\u2019ve been exposed to so much linguistic data that they\u2019re able to do very well without necessarily needing to do a lot of extrapolation beyond the kinds of data that they\u2019ve seen before. I have colleagues who have developed paradigms for measuring the extent to which they can extrapolate. They can actually do fairly well. You can do things like have them compose ideas together that they\u2019ve not encountered before and come up with new kinds of things when you put those pieces together. But I think that\u2019s something where there are still limitations in the systematicity of generalization. </p>\n<p>One of the things that surprises us about large language models is they sometimes behave in ways that make very little sense to us. That\u2019s because we\u2019re expecting them to generalize in ways that are like the ways that we\u2019re used to human beings generalizing. The other part of that, the learning part, I think, is perhaps one of the keys to thinking about how we can make systems that generalize better. Because being able to learn from less data very much requires being able to engage in good systematic generalizations. The way we talk about that in machine learning and in cognitive science is in terms of what we call inductive bias. </p>\n<p>Inductive bias is what the learner brings to a problem. That means that they favor some solutions over others. So if you see only a limited amount of data, there are many possible ways that you could explain [the] data that you saw. How do you choose between those many possible explanations? If you\u2019re learning a language, how do you choose which structure of the language you\u2019re going to infer?</p>\n<p>Inductive bias is the thing that breaks ties there. It tells us, \u201cYou should think about it this way rather than this way.\u201d Humans undoubtedly have a systematic set of inductive biases that are not instantiated in our neural networks. One of the important challenges for both AI and cognitive science is figuring out what those human inductive biases are and figuring out how to put them into things like these kinds of neural network systems. That\u2019s something that I work on in my lab. It\u2019s something that a lot of cognitive scientists are actively thinking about at the moment. </p>\n<p><strong>Sam Ransbotham:</strong> One of the things I enjoyed was with each chunk in your book, you take a little bit of a historical path through how we got to this point. You mentioned before Boole and the development of Boolean logic and Bayes, and how that added to the equation. Now I\u2019m going to be mean here and say that \u201cOK, these are some summaries of historical information leading to a path.\u201d My mean part is, \u201cWhat did you, Tom the human, add to this book?\u201d </p>\n<p>Were I to summarize the development of language or the development of Bayesian thought or probabilistic reasoning, and I put that in an LLM, and I ask, \u201cGive me four paragraphs about that.\u201d What did you the human add to this equation, to this book that would not have been done by that summarization process? </p>\n<p><strong>Tom Griffiths:</strong> There are a couple of things. One thing is the book actually involves a lot of primary source research. The story of where the book came from is partly that I realized that our field of cognitive science is one where, at the time when I started this project, many of the people who were there at the sort of birth of modern cognitive science in the 1950s were still alive, and I was able to go around and interview them and collect their stories. So a lot of the book is really telling those stories and using those to explain where those ideas come from. </p>\n<p>But in terms of the writing, I think the thing that I am able to do as a human author is engage in what we call theory of mind in cognitive science. This is me thinking about what it is that is going to make sense to a reader and that\u2019s going to be appealing, not just as a story but also clear in terms of conveying those ideas and putting those together in a structure that makes sense for the reader. I\u2019m not going to claim that\u2019s something only humans are going to be able to do forever. I think, at the moment, it\u2019s something that still humans are better at doing than the current models that we have. But we actually have been doing experiments in my lab and showing that large language models are not bad at putting together a curriculum for people to help them learn a concept, sort of figuring out, \u201cYou need to introduce this simpler idea first and then this other idea, and then it can put them together and so on.\u201d So they\u2019re definitely able to extract some of the structures that we use for solving these problems through our own intuitions about pedagogy and theory of mind and so on. </p>\n<p><strong>Sam Ransbotham:</strong> Maybe a different thing is that there [are] a lot of stories that you didn\u2019t include in the book. There\u2019s a lot of curation that\u2019s happening. That\u2019s one thing that I guess I\u2019m thinking about a lot. We have the machines capable of [having] effectively infinite memory of all possible stories. There\u2019s definitely value in you stitching out and saying, \u201cThese were important for this reason, and this is important for another reason,\u201d because these were some big steps. In doing so you\u2019ve inherently had to leave out some things. There\u2019s a curation going on there. I feel like that\u2019s something that you knew what to focus on that was important for the story, and maybe, like you say, the machines aren\u2019t quite there for that. </p>\n<p><strong>Tom Griffiths:</strong> This is actually, I think, a good connection to a question that I get asked a lot: What\u2019s going to happen to the kinds of jobs that humans do? Because we\u2019ve previously seen technology replace certain kinds of labor \u2014 physical labor [of] various kinds, manufacturing, things like that. And now we\u2019re seeing machines start to replace cognitive labor, which is different. </p>\n<p>As a psychologist, one of the things that I think about is there\u2019s another kind of labor that maybe is going to become even more important, which is metacognitive labor. This is what you\u2019re doing as a manager when you\u2019re thinking about, \u201cWho is going to be the best employee to do this job, and how should I describe it to them in order to ask them to do it so that it ends up being done in a way that\u2019s effective?\u201d Or thinking about for yourself: \u201cWhat strategy should I use to solve a problem?\u201d and \u201cWhat\u2019s the right way to approach this problem?\u201d</p>\n<p>What we\u2019re starting to do with these machines is outsource the cognitive component of the work. They\u2019re able to do some of that for us, but we\u2019re still having to do a lot of the metacognitive part. So that curation process you\u2019re describing is a good example of something [that] is not telling the story; it\u2019s figuring out the structure around that. That\u2019s going to be the analogs of the prompt that you\u2019d be providing to the AI system to maybe tell that story for you. </p>\n<p><strong>Sam Ransbotham:</strong> Are there other things besides metacognition? What else is in that list? </p>\n<p><strong>Tom Griffiths:</strong> The first thing, I would say, is metacognition is quite a big item. I\u2019m giving it the same status as physical labor and cognition. There are lots of people today whose jobs are metacognitive jobs, in management roles. And I think that\u2019s going to be a skill set that becomes more and more important. </p>\n<p>One of the things that happens in graduate school \u2014 well at least when I work with my graduate students \u2014 is not just learning how to do research but also learning how to think about what a good research project is. What I say to my students is there\u2019s a difference between the projects that we could do and the projects that we should do. Figuring out how to prioritize and work out what the best ideas are [is] one of the hardest things for people to learn, and that\u2019s part of why it takes quite a long time to do a Ph.D. I think that kind of skill set is going to be one that becomes increasingly valuable as it becomes easier to execute on these kinds of things. </p>\n<p><strong>Sam Ransbotham:</strong> One of the things that you mentioned and others have mentioned, too, is some of these constraints that are constraints on human intelligence are not the same as the constraints on machines. \u2026 What are some of those constraints [where] you see differences between machines and humans? And then what are the implications of there not being those constraints in the future? </p>\n<p><strong>Tom Griffiths:</strong> The three that I would normally highlight are: We have limited lifespans, limited time in this world. That means limited data we can learn from, limited compute. So [we have] limited cognitive resources, because we just carry around 2 or 3 pounds of neural tissue, and we have to do everything with that. </p>\n<p>And then, [we have] limited bandwidth for communication. If I want to share some of the data or compute that I have with you, I have to do it through this very inefficient mechanism that we\u2019re using right now of making honking noises. That set of constraints, I would say, [is] what makes human intelligence what it is, right? \u2026 We\u2019ve evolved minds in response to those constraints. </p>\n<p>If you look at what\u2019s going on for AI systems, really none of those things are true. We are able to turn up the knob of compute as high as it can go. It\u2019s somewhat limited at the moment because we\u2019re running out of money and energy resources to be able to keep building data centers. But that\u2019s something where the expectation that we should have is that we should, over time, continue to be able to have greater compute capacity for training the systems. That translates into being able to train systems on much more data, and all of [our] breakthrough AI systems have been trained on more data than human beings will ever experience. </p>\n<p>AlphaGo [has] many human lifetimes of playing [the] game of Go, and our language models today have many human lifetimes of linguistic data. </p>\n<p>And then bandwidth: You can take one AI system that\u2019s been trained on one set of things and then train it on some more things, or transfer the weights that it has between machines or split them up in all these ways. This idea of foundation models is that you can have one model and then copy it many times and then fine-tune those to solve different problems, and that\u2019s just fundamentally different from humans. For those reasons, I think we\u2019re going to see a meaningful divergence between the kinds of minds that humans are and the kinds of minds that the AI systems are. We shouldn\u2019t expect them to be the same because they\u2019re operating under different constraints, but we can still learn meaningful things about one another by comparing these different species when we take into account the fact that we\u2019ve evolved in these different environments. </p>\n<p><strong>Sam Ransbotham:</strong> I liked your \u201cWe communicate by making honking noises to each other.\u201d Is that holding us back? Is there a need for a language 3.0 type of a thing to move us to better bandwidth? </p>\n<p><strong>Tom Griffiths:</strong> Ren\u00e9 Descartes wrote about this idea all the way back when he was starting to think about math for the physical world. He talked about this idea that maybe there\u2019s a similar sort of structure to language. You could imagine creating a language where just hearing somebody say something in that language, you know what that thing is. </p>\n<p>In the same way that if I say \u201c10,000, 500, and 42,\u201d even though that\u2019s not a string you\u2019ve ever heard before, it tells you exactly the thing that it\u2019s referring to, you can sort of figure it out from the expression. </p>\n<p>[Gottfried] Leibniz was also obsessed with this idea. He had this idea that he called the universal character, which was a language in which you would be able to express things and then perform some mathematical operations on those things, and then figure out what the consequences of those things were. Just by expressing things in that language, you would know whether those things were true or false, and whether they were compatible with other things, and so on. That\u2019s what motivated him to think about mathematical logic. There was this spirit that traced through the book in terms of thinking about what the consequences of having these kinds of mathematical formalisms for understanding thought can be. </p>\n<p>So it\u2019s an interesting question: Can you turn all of that back around and come up with better languages for humans? There\u2019s a little bit of work along these lines. Bean Kim and colleagues at Google have a paper [that] looks at neologisms for language models, where it\u2019s looking at what are places where introducing a new term can help a language model capture a concept that\u2019s relevant to the operations that model is doing but also help us understand what it is that the language model is doing? </p>\n<p>I think you can imagine building better interfaces between us and AI systems by allowing language to evolve in ways that capture concepts that are relevant to those systems. That sounds like an interesting cognitive science problem. </p>\n<p><strong>Sam Ransbotham:</strong> We may be headed that way anyway by communicating with emojis. It doesn\u2019t convey well on this audio format, but the chapter you\u2019ve got about the representation of those universal symbols and how those worked, I didn\u2019t know about that beforehand. So I was able to pick up on that. One of the things on your website says \u2014 and I\u2019ll quote you \u2014 \u201cIt\u2019s natural to ask, \u2018What makes human intelligence special?\u2019\u201d So if it\u2019s natural, let me ask it: What makes human intelligence special? </p>\n<p><strong>Tom Griffiths:</strong> I would say those things that I mentioned, the constraints, are the things that really shape the nature of human intelligence. But I think it\u2019s maybe a mistake to think about that as being special. Rather, maybe we should think about that as being different. I think there\u2019s a tendency that people have when they talk about AI to think about intelligence being a one-dimensional scale. So people ask questions like, \u201cHave we made superintelligence?\u201d \u201cHave we made whatever the next iteration of this is?\u201d I think that\u2019s a very limited way of thinking about what it is minds are and what intelligence is. </p>\n<p>I think it\u2019s maybe better to think about intelligence as being shaped by the kinds of problems that minds have to solve and the kinds of constraints that they have to solve those problems under. That\u2019s maybe more like a sort of evolutionary way of looking at things, where we can imagine minds being shaped by those problems and constraints. But it\u2019s something where if we apply that to thinking about AI, we\u2019re going to have expectations that humans and AI systems are going to be meaningfully different. </p>\n<p>There are ways that you can imagine making AI systems better by incorporating things that come from people. That\u2019s part of what makes it exciting to think about these things from the perspective of cognitive science. But I don\u2019t think it\u2019s obligatory that we do that, because we could come up with completely different ways of solving those problems that make it possible for us to make AI systems that can do the things that we want AI systems to do, without them necessarily having to be exactly like us. </p>\n<p>I think when people talk about [artificial general intelligence] and wanting to make systems that are like people but better in whatever way, my reaction to that is to say, \u201cWell, maybe we should just think about them as being different from us but having a set of capabilities that that are perhaps harmonious with and complementary to the abilities that we have as humans.\u201d</p>\n<p>I think another thing that we should think about when we\u2019re trying to think about these differences between humans and AI systems is exactly what it is that we want to use our AI systems for. In general, we have a higher bar for the behavior of our AI systems than we have for a human being. That\u2019s appropriate, right? If we\u2019re going to be deploying a system intentionally and we could make it better, we should try and make it better. </p>\n<p>I think some of these things, like reasoning and being able to solve math problems, and so on, that are within the capacity of AI systems but are in some ways a challenge for our current large language model-based systems, are places where there\u2019s opportunities to use what people call a neuro-symbolic approach, where you build in aspects of logic or other kinds of mathematical tools that these systems can use to be able to do things that make them better than the baseline language models are. That\u2019s an important thing to do if you want to make better AI systems.</p>\n<p>It\u2019s not necessarily something that\u2019s going to help us understand human minds better. Human minds are built out of the same sort of messy stuff that we\u2019re trying to build our large language models out of and mess up in the same kinds of ways as those models when you have them solve math problems or do other kinds of things. Part of that is suggesting that, again, there\u2019s a kind of divergence that could happen where we need to do a bunch more work to figure out how to make our AI systems reliable in ways \u2026 in order to be deployable, but we have enough of the basic principles figured out that we still have good insight into human cognition already.</p>\n<p><strong>Sam Ransbotham:</strong> What I like about that is it speaks to the idea that LLMs, though wonderful, [are] not necessarily the end of the chapter of the book. There are whole different ways of approaching these problems that may or may not have strengths and weaknesses. Just like you mentioned with the growth of the different approaches, whether it\u2019s symbols and rules or Bayesian logic or neural networks, we can realize some limitations and then build on them and try to combine those in different ways. So maybe there\u2019s hope for some entirely new approaches. </p>\n<p>This is all interesting. Given these laws of thought, what should our listeners change about anything that they do tomorrow? </p>\n<p><strong>Tom Griffiths:</strong> I think one thing is it might change the way that you think about what AI systems are doing. I think we all have a model of how minds work that\u2019s based on interacting with other humans. And when we think about our AI systems, we think about them using the same set of tools that we use for thinking about other humans. And that can be \u2026 misleading in a few ways. </p>\n<p>One way is that we can have incorrect assumptions about how they\u2019re going to generalize. We say, \u201cThis AI system solved this olympiad math problem that\u2019s extremely hard.\u201d And then if we think about a human being who is able to do that, you\u2019d say, \u201cOh, they must be incredibly smart. They must be able to do all sorts of things that I can\u2019t do.\u201d But in fact, that\u2019s a relatively narrow piece of the profile of these systems. In some ways, getting better at solving these kinds of problems might make them less good at solving other kinds of problems, and there\u2019s a kind of balancing act that goes on there. </p>\n<p>So when you try and make generalizations [such as], \u201cIf this kind of machine can solve this problem, it\u2019s going to be able to solve this other problem,\u201d I think if you approach it as not something that\u2019s like us but rather something that\u2019s shaped by the way in which it\u2019s been trained and the constraints that it operates under, and all these other kinds of things, our expectations about how the systems are going to generalize would be different. So that might make you a little more pessimistic about timelines for building [artificial general intelligence], right? We shouldn\u2019t make the same generalizations from the peaks, covering the entire surface of the abilities of these systems. </p>\n<p>I think the other thing it might do is help us to imagine futures that are perhaps less scary in terms of the way that we imagine these systems affecting human societies. If we start thinking about AI systems as being different from us, then that suggests this view of complementarity, where there are going to be things that you\u2019re going to be better at, and there are going to be things the AI is going to be better at. Just like you try and figure out how to divide jobs up across people who have different kinds of abilities, thinking about how it is that we\u2019re going to divide the kinds of things that we want to be able to accomplish between humans who can do certain kinds of things and AI systems that can do other kinds of things is maybe a healthier way of thinking about this than sort of imagining that we\u2019re going to be completely replaced. </p>\n<p><strong>Sam Ransbotham:</strong> It\u2019s fascinating talking to you. We\u2019ve been talking about the <cite>Laws of Thought</cite>, which is coming out [on] Feb. 10. Tom, thanks for taking the time to talk with us today. </p>\n<p><strong>Tom Griffiths:</strong> Thank you. </p>\n<p><strong>Sam Ransbotham:</strong> Thanks for listening today. On our next episode, I\u2019ll speak with Nobel Prize-winning economist Daron Acemoglu about his research at MIT. Please join us.</p>\n<p><strong>Allison Ryder:</strong> Thanks for listening to <cite>Me, Myself, and AI</cite>. Our show is able to continue, in large part, due to listener support. Your streams and downloads make a big difference. If you have a moment, please consider leaving us an Apple Podcasts review or a rating on Spotify. And share our show with others you think might find it interesting and helpful.</p>\n<p></p>",
      "tier": "trending",
      "selection_reason": "Theoretical foundations of AI and cognition",
      "audience_value": "Deep insights into AI theory and development",
      "urgency_score": 6,
      "category_tag": "\ud83d\ude80 AI & Innovation"
    }
  ]
}